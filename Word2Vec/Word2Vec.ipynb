{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4CZxkSb72gr",
        "colab_type": "text"
      },
      "source": [
        "### <b>Word2Vec</b>\n",
        "\n",
        "<br>\n",
        "\n",
        "##### What is Word2Vec ? \n",
        "\n",
        "Word2Vec is a Word Embedding that´s applied on NLP tasks, across Embedding We be able extract representation on words in text, Beyond be most user Embedding for NLP, let´s go studing this concept. \n",
        " \n",
        "<br>\n",
        "\n",
        "Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one another in the space.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "#### How does Word2Vec work?\n",
        "\n",
        "Word2Vec is a method to construct such an embedding. It can be obtained using two methods (both involving Neural Networks): Skip Gram and Common Bag Of Words (CBOW)\n",
        "CBOW Model: This method takes the context of each word as the input and tries to predict the word corresponding to the context. Consider our example: Have a great day.\n",
        "Let the input to the Neural Network be the word, great. Notice that here we are trying to predict a target word (day) using a single context input word great. More specifically, we use the one hot encoding of the input word and measure the output error compared to one hot encoding of the target word (day). In the process of predicting the target word, we learn the vector representation of the target word.\n",
        "Let us look deeper into the actual architecture.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "<p align=center>\n",
        "<img src=\"https://miro.medium.com/max/700/0*3DFDpaXoglalyB4c.png\" width=\"70%\"></p>\n",
        "\n",
        "<br>\n",
        "\n",
        "The input or the context word is a one hot encoded vector of size V. The hidden layer contains N neurons and the output is again a V length vector with the elements being the softmax values.\n",
        "Let’s get the terms in the picture right:\n",
        "- Wvn is the weight matrix that maps the input x to the hidden layer (V*N dimensional matrix)\n",
        "-W`nv is the weight matrix that maps the hidden layer outputs to the final output layer (N*V dimensional matrix)\n",
        "I won’t get into the mathematics. We’ll just get an idea of what’s going on.\n",
        "The hidden layer neurons just copy the weighted sum of inputs to the next layer. There is no activation like sigmoid, tanh or ReLU. The only non-linearity is the softmax calculations in the output layer.\n",
        "But, the above model used a single context word to predict the target. We can use multiple context words to do the same.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "<p align=center>\n",
        "<img src=\"https://miro.medium.com/max/596/0*CCsrTAjN80MqswXG\" width=\"70%\"></p>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "The above model takes C context words. When Wvn is used to calculate hidden layer inputs, we take an average over all these C context word inputs.\n",
        "So, we have seen how word representations are generated using the context words. But there’s one more way we can do the same. We can use the target word (whose representation we want to generate) to predict the context and in the process, we produce the representations. Another variant, called Skip Gram model does this.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Skip-Gram model\n",
        "\n",
        "\n",
        "This looks like multiple-context CBOW model just got flipped. To some extent that is true.\n",
        "We input the target word into the network. The model outputs C probability distributions. What does this mean?\n",
        "For each context position, we get C probability distributions of V probabilities, one for each word.\n",
        "\n",
        "\n",
        "<br>\n",
        "<p align=center>\n",
        "<img src=\"https://miro.medium.com/max/700/0*Ta3qx5CQsrJloyCA.png\" width=\"70%\"></p>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "#### Who wins?\n",
        "\n",
        "Both have their own advantages and disadvantages. According to Mikolov, Skip Gram works well with small amount of data and is found to represent rare words well.\n",
        "On the other hand, CBOW is faster and has better representations for more frequent words.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aElA6-d4z2n",
        "colab_type": "text"
      },
      "source": [
        "* usar string.puctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTrKaoA47nmt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "5473f4a7-50ae-4f0d-c84f-e76074d600da"
      },
      "source": [
        "import os \n",
        "import re \n",
        "import string \n",
        "import time \n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns \n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, GRU\n",
        "from tensorflow.keras.layers import Dropout, InputLayer, Bidirectional\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.losses import BinaryCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.models import model_from_json"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubE6a0E2U4SZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# seed \n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhR8C1ReA2dm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "01eb5873-77c3-4fcb-9734-47973c064719"
      },
      "source": [
        "path = '/content/drive/My Drive/Deep Learning - Projetos/Embeddings /Musical_instruments_reviews.csv'\n",
        "data = pd.read_csv(path)\n",
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>asin</th>\n",
              "      <th>reviewerName</th>\n",
              "      <th>helpful</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>overall</th>\n",
              "      <th>summary</th>\n",
              "      <th>unixReviewTime</th>\n",
              "      <th>reviewTime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A2IBPI20UZIR0U</td>\n",
              "      <td>1384719342</td>\n",
              "      <td>cassandra tu \"Yeah, well, that's just like, u...</td>\n",
              "      <td>[0, 0]</td>\n",
              "      <td>Not much to write about here, but it does exac...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>good</td>\n",
              "      <td>1393545600</td>\n",
              "      <td>02 28, 2014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A14VAT5EAX3D9S</td>\n",
              "      <td>1384719342</td>\n",
              "      <td>Jake</td>\n",
              "      <td>[13, 14]</td>\n",
              "      <td>The product does exactly as it should and is q...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Jake</td>\n",
              "      <td>1363392000</td>\n",
              "      <td>03 16, 2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A195EZSQDW3E21</td>\n",
              "      <td>1384719342</td>\n",
              "      <td>Rick Bennette \"Rick Bennette\"</td>\n",
              "      <td>[1, 1]</td>\n",
              "      <td>The primary job of this device is to block the...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>It Does The Job Well</td>\n",
              "      <td>1377648000</td>\n",
              "      <td>08 28, 2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A2C00NNG1ZQQG2</td>\n",
              "      <td>1384719342</td>\n",
              "      <td>RustyBill \"Sunday Rocker\"</td>\n",
              "      <td>[0, 0]</td>\n",
              "      <td>Nice windscreen protects my MXL mic and preven...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>GOOD WINDSCREEN FOR THE MONEY</td>\n",
              "      <td>1392336000</td>\n",
              "      <td>02 14, 2014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A94QU4C90B1AX</td>\n",
              "      <td>1384719342</td>\n",
              "      <td>SEAN MASLANKA</td>\n",
              "      <td>[0, 0]</td>\n",
              "      <td>This pop filter is great. It looks and perform...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>No more pops when I record my vocals.</td>\n",
              "      <td>1392940800</td>\n",
              "      <td>02 21, 2014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       reviewerID        asin  ... unixReviewTime   reviewTime\n",
              "0  A2IBPI20UZIR0U  1384719342  ...     1393545600  02 28, 2014\n",
              "1  A14VAT5EAX3D9S  1384719342  ...     1363392000  03 16, 2013\n",
              "2  A195EZSQDW3E21  1384719342  ...     1377648000  08 28, 2013\n",
              "3  A2C00NNG1ZQQG2  1384719342  ...     1392336000  02 14, 2014\n",
              "4   A94QU4C90B1AX  1384719342  ...     1392940800  02 21, 2014\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBCoShPECDLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lower case columns \n",
        "data.columns = data.columns.str.lower()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLrbh4SoDONJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drop columns \n",
        "cols_drop = ['reviewerid','asin','reviewername','helpful','unixreviewtime','reviewtime']\n",
        "data.drop(columns=cols_drop, axis=1, inplace=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1furEdWEH9j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4e2cab2f-af75-40e1-e071-14dfb3f5b3a5"
      },
      "source": [
        "# Summary \n",
        "data['overall'] = data['overall'].replace({1:2,4:5})\n",
        "\n",
        "\n",
        "# sentiments \n",
        "data['overall'] = data['overall'].replace({2:'Negative',\n",
        "                                           3:'Neutral',\n",
        "                                           5:'Positive'\n",
        "                                           })\n",
        "\n",
        "data['overall'].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Positive    9022\n",
              "Neutral      772\n",
              "Negative     467\n",
              "Name: overall, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXa_x5FpcEa9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['reviewtext'] = data['reviewtext'].astype(str)\n",
        "data['overall'] = data['overall'].astype(str)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV0Zb0thBk2t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "108a866d-02c7-4b8a-a7eb-b016664fe71d"
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "reviewtext    0\n",
              "overall       0\n",
              "summary       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUwKxQ-MB-EG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.dropna(axis=0, inplace=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK7emPFcMNGz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "31b0da8e-2b20-4e5c-ebf7-2c9b63bb3ca5"
      },
      "source": [
        "data['reviewtext'] = data['reviewtext'] + ' ' + data['summary']\n",
        "data.drop('summary', axis=1, inplace=True)\n",
        "data.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewtext</th>\n",
              "      <th>overall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Not much to write about here, but it does exac...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The product does exactly as it should and is q...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The primary job of this device is to block the...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Nice windscreen protects my MXL mic and preven...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>This pop filter is great. It looks and perform...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          reviewtext   overall\n",
              "0  Not much to write about here, but it does exac...  Positive\n",
              "1  The product does exactly as it should and is q...  Positive\n",
              "2  The primary job of this device is to block the...  Positive\n",
              "3  Nice windscreen protects my MXL mic and preven...  Positive\n",
              "4  This pop filter is great. It looks and perform...  Positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa3YFmOcNssk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16e9b985-e52c-446b-eeda-a26ca047e2b2"
      },
      "source": [
        "X = data['reviewtext']\n",
        "y = data['overall']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=42)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "y_test = encoder.transform(y_test)\n",
        "\n",
        "print('Classes: ', encoder.inverse_transform([0,1,2]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classes:  ['Negative' 'Neutral' 'Positive']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw2FO5zUEzW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feature engineering for NLP \n",
        "\n",
        "def removing_noise(text):\n",
        "      \n",
        "    removing_list = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "    text = re.sub(removing_list, \" \", str(text))\n",
        "    text = re.sub(\"'\", ' ', text)\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    return text \n",
        "\n",
        "\n",
        "\n",
        "def lemmatization(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = lemmatizer.lemmatize(text)\n",
        "    return text \n",
        "\n",
        "\n",
        "\n",
        "def tokenization(X_train, X_test, max_sequence_length=10, words_token=10000):\n",
        "\n",
        "      tokenizer = Tokenizer(num_words=words_token)\n",
        "      tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "      word_index = tokenizer.word_index\n",
        "      num_words = len(word_index) + 1 \n",
        "\n",
        "      sequences_train = tokenizer.texts_to_sequences(X_train)\n",
        "      sequences_test =  tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "      X_train = pad_sequences(sequences_train, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "      X_test = pad_sequences(sequences_test, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "\n",
        "      return (X_train, X_test, num_words)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "def stop_words(text):\n",
        "\n",
        "    stop_list = set(stopwords.words('english'))\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "      if token not in stop_list:\n",
        "        tokens.append(token)\n",
        "        return \" \".join(tokens)\n",
        "      else: \n",
        "        pass "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqO2bqgfpTNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['reviewtext'] = data['reviewtext'].apply(lambda x: removing_noise(x))\n",
        "data['reviewtext'] = data['reviewtext'].apply(lambda x: stop_words(x))\n",
        "data['reviewtext'] = data['reviewtext'].apply(lambda x: lemmatization(x))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAu0ukybXj-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CountVectorizer \n",
        "cv = CountVectorizer(tokenizer=word_tokenize)\n",
        "X_train_cv = cv.fit_transform(X_train)\n",
        "X_test_cv = cv.transform(X_test)\n",
        "\n",
        "\n",
        "# TF-IDF \n",
        "tfidf = TfidfTransformer()\n",
        "X_train_idf = tfidf.fit_transform(X_train_cv)\n",
        "X_test_idf = tfidf.transform(X_test_cv)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yECMJEkOOGBh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "a11a771c-58e6-420d-bc75-1d19fd74a33f"
      },
      "source": [
        "mdl = MultinomialNB()\n",
        "mdl.fit(X_train_idf, y_train)\n",
        "y_pred = mdl.predict(X_test_idf)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       139\n",
            "           1       0.00      0.00      0.00       229\n",
            "           2       0.88      1.00      0.94      2711\n",
            "\n",
            "    accuracy                           0.88      3079\n",
            "   macro avg       0.29      0.33      0.31      3079\n",
            "weighted avg       0.78      0.88      0.82      3079\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dLOOPzT9tOG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c3fd5608-8ed8-4299-c743-cfd907b15e12"
      },
      "source": [
        "mdl = XGBClassifier(random_state=42)\n",
        "mdl.fit(X_train_idf, y_train)\n",
        "y_pred = mdl.predict(X_test_idf)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.02      0.04       139\n",
            "           1       0.58      0.08      0.14       229\n",
            "           2       0.89      1.00      0.94      2711\n",
            "\n",
            "    accuracy                           0.88      3079\n",
            "   macro avg       0.66      0.37      0.37      3079\n",
            "weighted avg       0.85      0.88      0.84      3079\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDbwSGu-Dsck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, num_words = tokenization(X_train, X_test, max_sequence_length=50, words_token=10000)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkGxY0S-EEOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_sequence_length = 50"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz4FT9nED0wN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "c0bd7378-6c4b-404e-bdcf-226a2136bf5b"
      },
      "source": [
        "# LSTM with Embedding \n",
        "\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=max_sequence_length))\n",
        "model.add(Embedding(input_dim=num_words,\n",
        "                    output_dim=300,\n",
        "                    input_length=max_sequence_length))\n",
        "model.add(Dropout(0.20))\n",
        "model.add(LSTM(128, recurrent_dropout=0.20))\n",
        "model.add(LSTM(128, recurrent_dropout=0.20))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.20))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 50, 300)           5545500   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 50, 300)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 256)               439296    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 6,001,439\n",
            "Trainable params: 6,001,439\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Afzjvr-Ecoy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "13b4df23-8365-4a1d-b18f-3343b761e292"
      },
      "source": [
        "model.compile(optimizer=RMSprop(0.001),\n",
        "              loss=SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "57/57 [==============================] - 52s 906ms/step - loss: 0.4719 - accuracy: 0.8687 - val_loss: 0.4045 - val_accuracy: 0.8805\n",
            "Epoch 2/10\n",
            "57/57 [==============================] - 51s 893ms/step - loss: 0.3577 - accuracy: 0.8807 - val_loss: 0.4432 - val_accuracy: 0.8815\n",
            "Epoch 3/10\n",
            "57/57 [==============================] - 51s 896ms/step - loss: 0.2861 - accuracy: 0.8940 - val_loss: 0.3777 - val_accuracy: 0.8737\n",
            "Epoch 4/10\n",
            "57/57 [==============================] - 51s 894ms/step - loss: 0.2235 - accuracy: 0.9138 - val_loss: 0.5062 - val_accuracy: 0.8756\n",
            "Epoch 5/10\n",
            "57/57 [==============================] - 51s 894ms/step - loss: 0.1657 - accuracy: 0.9379 - val_loss: 0.5825 - val_accuracy: 0.7908\n",
            "Epoch 6/10\n",
            "57/57 [==============================] - 51s 892ms/step - loss: 0.1159 - accuracy: 0.9612 - val_loss: 0.5424 - val_accuracy: 0.8675\n",
            "Epoch 7/10\n",
            "57/57 [==============================] - 51s 895ms/step - loss: 0.0732 - accuracy: 0.9749 - val_loss: 0.6540 - val_accuracy: 0.8685\n",
            "Epoch 8/10\n",
            "57/57 [==============================] - 51s 894ms/step - loss: 0.0459 - accuracy: 0.9854 - val_loss: 0.9195 - val_accuracy: 0.7925\n",
            "Epoch 9/10\n",
            "57/57 [==============================] - 51s 902ms/step - loss: 0.0314 - accuracy: 0.9908 - val_loss: 0.8708 - val_accuracy: 0.8701\n",
            "Epoch 10/10\n",
            "57/57 [==============================] - 54s 948ms/step - loss: 0.0193 - accuracy: 0.9943 - val_loss: 0.8145 - val_accuracy: 0.8594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq-GIJXMGZpN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ebcb5208-e034-43a5-d384-4188b398f979"
      },
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97/97 [==============================] - 5s 49ms/step - loss: 0.8145 - accuracy: 0.8594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiI6vuIRHUoz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "edf0ff69-9fb3-41ac-857b-b9ce7db9e458"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.20      0.24       139\n",
            "           1       0.31      0.25      0.28       229\n",
            "           2       0.91      0.94      0.93      2711\n",
            "\n",
            "    accuracy                           0.86      3079\n",
            "   macro avg       0.51      0.47      0.48      3079\n",
            "weighted avg       0.84      0.86      0.85      3079\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}