# -*- coding: utf-8 -*-
"""Test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wEs7we88DO7mdYUcnXyWi0b05juj9oyA
"""

!pip install -q pyyaml h5py
!pip install emoji

# Commented out IPython magic to ensure Python compatibility.
import re
import string
import random 
import time 
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns 
import emoji 
from PIL import  Image

# %matplotlib inline 
import warnings
warnings.filterwarnings('ignore')


import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from wordcloud import WordCloud

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder


import tensorflow as tf 
from tensorflow import keras 
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import save_model, load_model
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.optimizers import Adam, RMSprop, SGD
from tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy
from tensorflow.keras.layers import Flatten, Embedding, Dropout
from tensorflow.keras.layers import Conv1D, SpatialDropout1D
from tensorflow.keras.layers import Dense, Input 
from tensorflow.keras.layers import GlobalAveragePooling1D
from tensorflow.keras.layers import LSTM, Dropout, GRU, Bidirectional

# GPU with TensorFlow 

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

# set global seed 
seed = 42
np.random.seed(seed)
tf.random.set_seed(seed)

path = '/content/drive/My Drive/Deep Learning - Projetos/Classificação de Texto - Twitter /training.1600000.processed.noemoticon.csv'
data = pd.read_csv(path, encoding='latin', header=None)

data.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']
data.head()

data.drop(['id', 'date', 'query', 'user_id'], axis=1, inplace=True)

data['sentiment'] = data['sentiment'].astype(str)
data['text'] = data['text'].astype(str)

dicio = {'0':'Negative', '4':'Positive'}
data['sentiment'] = data['sentiment'].map(dicio)

# Cleaning text 

def cleaning_text(text):

  """
  Cleaning text in Twetts 
  Removing unwanted characters and emojis
                 
                                          """
  
  # Removing characters and emojis 
  removing_list = "@\S+|https?:\S+|http?:\S|[^A-Za-z0-9]+"
  emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U0001F1F2-\U0001F1F4"  # Macau flag
        u"\U0001F1E6-\U0001F1FF"  # flags
        u"\U0001F600-\U0001F64F"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U0001F1F2"
        u"\U0001F1F4"
        u"\U0001F620"
        u"\u200d"
        u"\u2640-\u2642"
        "]+", flags=re.UNICODE)

  text = emoji_pattern.sub(r'', str(text))
  text = re.sub(removing_list, " ", text)
  text = re.sub(r'\W+', ' ', text)
  text = re.sub("'", '', text)
  text = text.lower().strip()


  # Stemming and Stopwords 
  stemmer = SnowballStemmer('english')
  stop_words = set(stopwords.words('english'))

  tokens = []
  for token in text.split():
    if token not in stop_words:
      tokens.append(stemmer.stem(token))
    else: 
      pass 


  return " ".join(tokens)

# cleaning text 
data['text'] = data['text'].apply(lambda x: cleaning_text(x))

# spliting data 
X = data['text']
y = data['sentiment']

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=42)

# LabelEncoder 
encoder = LabelEncoder()
y_train = encoder.fit_transform(y_train.to_list())
y_test = encoder.fit_transform(y_test.to_list())

y_train = y_train.reshape(-1,1)
y_test = y_test.reshape(-1,1)
  

print('Classes: ', encoder.inverse_transform([0,1]))
print('\n')


# Tokenization 
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)


# Vocabulary Size 
word_index = tokenizer.word_index
num_words = len(word_index) + 1 

# Tokens 
sequence_train = tokenizer.texts_to_sequences(X_train)
sequence_test = tokenizer.texts_to_sequences(X_test)

# max sequence 
MAX_SEQUENCE_LENGTH = 55
   
# pad_sequences 
X_train = pad_sequences(sequences=sequence_train, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')
X_test = pad_sequences(sequences=sequence_test, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

# compile parameters
optimizer = RMSprop(learning_rate=0.001)
loss = BinaryCrossentropy()
metrics = ['accuracy']

# Embedding parameters

EMBEDDING_GLOVE = 300
WORD_INDEX = word_index
EMBEDDING_DIM = 100
MAX_SEQUENCE_LENGTH = 55
NUM_WORDS = 227664

# NEW LSTM 
model = Sequential()
model.add(Embedding(input_dim=NUM_WORDS,
                    output_dim=EMBEDDING_DIM,
                    input_length=MAX_SEQUENCE_LENGTH))
model.add(SpatialDropout1D(0.20))
model.add(Conv1D(128, kernel_size=5, strides=1, padding='valid', activation='relu')) 
model.add(LSTM(units=128, return_sequences=True, recurrent_dropout=0.20))
model.add(LSTM(units=64, return_sequences=True, recurrent_dropout=0.20))
model.add(GlobalAveragePooling1D())
model.add(Dense(units=512, activation='relu'))
model.add(Dropout(0.20))
model.add(Dense(units=64, activation='relu'))
model.add(Dense(units=1, activation='sigmoid'))

model.compile(optimizer=optimizer,
              loss=loss,
              metrics=metrics)

model.summary()

# callbacks

checkpoint = ModelCheckpoint(filepath='model.h5',
                             monitos='val_loss',
                             verbose=1,
                             save_only_weights=True)


early_stopping = EarlyStopping(monitor='val_loss', 
                               min_delta=0.00001,
                               patience=10)


reduce_learning_rate = ReduceLROnPlateau(monitor='val_loss',
                                         factor=0.2,
                                         patience=5,
                                         min_delta=0.0001)



callbacks = [checkpoint, early_stopping, reduce_learning_rate]

# Commented out IPython magic to ensure Python compatibility.
# %%time 
# 
# model.fit(X_train, y_train,
#           batch_size=1024,
#           epochs=1,
#           validation_data=(X_test, y_test),
#           callbacks=[callbacks])

class_names = ['Negative', 'Positive']

text = 'I am glad to know you!!!'

# time 
time_pred = time.time()

# preprocessing inference 
text = [text]
sequence_text = tokenizer.texts_to_sequences(text)
text = pad_sequences(sequence_text, maxlen=MAX_SEQUENCE_LENGTH)

# prediction 
sentiment = model.predict(text)


if sentiment >= 0.50: 
  print('Sentiment: Positive')
  print('Probability: {}'.format(sentiment))
  print('Inference Time: {}'.format(time_pred))
elif sentiment < 0.50:
  print('Sentiment: Negative')
  print('Probability: {}'.format(sentiment[1]*100))
  print('Inference Time: {}'.format(time_pred))
else:
  pass

def predict_class(text):

  # preprocessing inference 
  text = [text]
  sequence_text = tokenizer.texts_to_sequences(text)
  text = pad_sequences(sequence_text, maxlen=MAX_SEQUENCE_LENGTH)

  # prediction 
  sentiment = model.predict(text)
  sentiment = np.argmax(sentiment, axis=1)

  if sentiment == 0:
    print('Negative')
  elif sentimet ==1:
    print('Positive')
  else: 
    pass

frase = 'Had I very strong loss, my car are stroke'

predict_class(text=frase)

# save model
model.save_weights('model.h5')