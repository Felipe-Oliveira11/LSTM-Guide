# -*- coding: utf-8 -*-
"""Glove-Embedding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NuYW5h6OyvXRYjdbbPZdSD9wwmNtmeIo
"""

# Commented out IPython magic to ensure Python compatibility.
import os 
import re
import string
import random 
import time 
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns 

# %matplotlib inline 
import warnings
warnings.filterwarnings('ignore')


import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from wordcloud import WordCloud

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder

import tensorflow as tf 
from tensorflow import keras 
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.layers import Flatten, Embedding, Dropout
from tensorflow.keras.layers import Conv1D, SpatialDropout1D
from tensorflow.keras.layers import Dense, Input 
from tensorflow.keras.layers import GlobalMaxPooling2D, GlobalAveragePooling1D
from tensorflow.keras.layers import LSTM, Dropout, GRU, Bidirectional

np.random.seed(42)
tf.random.set_seed(42)

path = '/content/drive/My Drive/Deep Learning - Projetos/Classificação de Texto - Twitter /training.1600000.processed.noemoticon.csv'
data = pd.read_csv(path, encoding='latin', header=None)

data.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']
data.head()

data.drop(['id', 'date', 'query', 'user_id'], axis=1, inplace=True)

# Regex sub

text_cleaning_re = "@\S+|https?:\S+|http?:\S|[^A-Za-z0-9]+"
texto = 'Felipe@ foi a escola@#4 ontem*['

limpeza = re.sub(text_cleaning_re ,' ', texto)

print(texto,'\n',limpeza)

# Stemmer and Stopwords
stop_words = stopwords.words('english')
stemmer = SnowballStemmer('english')

text_cleaning_re = "@\S+|https?:\S+|http?:\S|[^A-Za-z0-9]+"


def preprocess(text, stem=False):
  text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()
  tokens = []
  for token in text.split():
    if token not in stop_words:
      if stem:
        tokens.append(stemmer.stem(token))
      else: 
        tokens.append(token)
  return " ".join(tokens)

data['text'] = data['text'].apply(lambda x: preprocess(x, stem=False))

# train and test 

X = data['text']
y = data['sentiment']


X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=42)


encoder = LabelEncoder()
y_train = encoder.fit_transform(y_train)
y_test = encoder.fit_transform(y_test)

print('Train: {}'.format(X_train.shape))
print('Teste: {}'.format(X_test.shape))

# Tokenizador 
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

# Índice de palavras 
word_index = tokenizer.word_index
vocab_size = len(word_index) + 1
print('Vocabulary size: {}'.format(vocab_size))

max_sequence_length = 50 

sequences_train = tokenizer.texts_to_sequences(X_train)
sequences_test = tokenizer.texts_to_sequences(X_test)

X_train = pad_sequences(sequences_train, maxlen=max_sequence_length, padding='post')
X_test = pad_sequences(sequences_test, maxlen=max_sequence_length, padding='post')


print('Maior Sequência: {}'.format(len(max(data.text))))
print('Sequência definida: {}'.format(max_sequence_length))

# identifiando sentimentos [Positivo = 1 | Negativo = 0]
for x,y in zip(y_train[0:3], X_train[0:3]):
  print('Sentiment {} ----- {}'.format(x,y))

# índice de palavras
word_index

# De texto para token
for x,y in zip(data.text[20:22], sequences_test):
  print('{}. --> {}.'.format(x,y))

"""<br>
<hr>
<br>


### Embedding - Glove 


<br>
"""

# Word Embedding at Stanford
!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove.6B.zip

# Glove Embedding
GLOVE_EMB = '/content/glove.6B.300d.txt'
EMBEDDING_DIM = 300

"""calculamos um índice de mapeamento de palavras para embeddings conhecidos, analisando o despejo de dados de embeddings pré-treinados"""

# preparing Embedding 

embeddings_index = {}

f = open(GLOVE_EMB)

for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Encontrado {} vetores de palavras.'.format(len(embeddings_index)))

"""podemos aproveitar nosso embedding_index dicionário e nosso word_index para calcular nossa matriz de incorporação.


* Criando Matrix de Embedding (Glove) 

<br>
"""

embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))

for word, i in word_index.items():
  embedding_vector = embeddings_index.get(word)
  if embedding_vector is not None:
    embedding_matrix[i] = embedding_vector

"""Carregamos essa matriz de incorporação (Embedding Matrix) em uma Embedding camada. Observe que definimos <b>trainable=False</b> para evitar que os pesos sejam atualizados durante o treinamento.


* trainable definido como true, os pesos seriam atualizados do modelo Glove

<br>
"""

# Embedding layer (Glove)

embedding_layer = Embedding(vocab_size,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=max_sequence_length,
                            trainable=False)



# Embedding layer (Not pre-trained)
"""
embedding_layer = Embedding(vocab_size,
                            EMBEDDING_DIM,
                            input_length=max_sequence_length)"""

# LSTM 

model = Sequential()
model.add(Input(shape=max_sequence_length))
model.add(embedding_layer)
model.add(SpatialDropout1D(0.20))
model.add(LSTM(units=64, recurrent_dropout=0.20))
model.add(Dense(units=512, activation='relu'))
model.add(Dropout(0.20))
model.add(Dense(units=512, activation='relu'))
model.add(Dense(units=1, activation='sigmoid'))

model.summary()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# 
# model.compile(optimizer=Adam(0.001),
#               loss=BinaryCrossentropy(),
#               metrics=['accuracy'])
# 
# 
# history = model.fit(X_train, y_train,
#                     batch_size=1024,
#                     epochs=5,
#                     validation_data=(X_test, y_test))

"""<br>
<hr>
<hr>
<br>
"""